{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949eeff4-ef8b-4602-bc49-ef7ee36492c3",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f96c8-bb66-4688-88d0-6d1260fc6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import ee\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import seaborn as sns\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Sign up for google earth engine\n",
    "# https://accounts.google.com/v3/signin/identifier?continue=https%3A%2F%2Fuc.appengine.google.com%2F_ah%2Fconflogin%3Fstate%3D%7EAJKiYcGTAqfnLHQ3IF0PRvxyDVDHOzQi3yIk92Z-tP7i6cyDz1qVoE4NaKkTL5gVN9MtRNpdCih_DPZ3-cleXMSfg7lbQn5M6ZFIawvFwRVMuRY-XUXstvtr8yOEmBXZxVFwrDS2MfySBsbdR-TyXZjp5m3DySECt4X2Zn55Y8Kj3Fg4INJl9ztrjc6twcmkkvtM35aqc23L&ifkv=ARZ0qKJ119_csxBqwfHRZLfiV0_DiUA_T-yvNLfs8hD5uHR4WTitbIsQ_us3PSxBY1yOnVB9TLuqug&passive=true&flowName=GlifWebSignIn&flowEntry=ServiceLogin&dsh=S-1895545422%3A1712087817582842&theme=mn&ddm=0\n",
    "\n",
    "ee.Authenticate()\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96c64d-3bcf-4c4b-90eb-bd6d228d3e45",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87f969-aefa-41cc-b49e-ab9a308825da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph of fire perimeter - 4days\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in the shapefile\n",
    "file = \"C:\\\\PATH\\\\TO\\\\FIRESPREAD\\\\SHP\\\\FireSpreadPoly_4days_simple.shp\"\n",
    "data = gpd.read_file(file)\n",
    "\n",
    "# Sort by date\n",
    "data_sorted = data.sort_values('GDB_TO_DAT')\n",
    "xmin, ymin, xmax, ymax = data_sorted.total_bounds\n",
    "# Create a 2x2 subplot figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Loop over each day (0-3) and plot the fire spread on the corresponding subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # Subset the data to only include the current day\n",
    "    data_subset = data_sorted.iloc[[i]]\n",
    "    # Plot the fire spread on the current subplot\n",
    "    data_subset.plot(column='GDB_TO_DAT', alpha=0.9,color=\"red\", ax=ax)\n",
    "    # Set the title of the subplot to the current day\n",
    "    ax.set_title(f\"Day {i}\")\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    # Label the y-axis as \"Latitude\"\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"C:\\\\PATH\\\\TO\\\\SAVE\\\\IMAGE\\\\fourpanel_fireprogression.jpg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff4bf4-b4a2-4ce6-9754-d7c15b221f5f",
   "metadata": {},
   "source": [
    "# Visualize the identification of held and breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a7cb4-2aae-423c-a4ca-9b4adf4365de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Day current \n",
    "# Day Next\n",
    "i=1\n",
    "w=2\n",
    "# buffer to remove edge effects \n",
    "meters = 300\n",
    "\n",
    "#########################\n",
    "file = \"C:\\\\PATH\\\\TO\\\\FIRESPREAD\\\\SHP\\\\FireSpreadPoly_4days_simple.shp\"\n",
    "# Read in the shapefile \n",
    "data = gpd.read_file(file)\n",
    "\n",
    "# Subset the data GeoDataFrame to only include the first row\n",
    "data_subset = data.iloc[[w]]\n",
    "\n",
    "xmin, ymin, xmax, ymax = data.total_bounds\n",
    "latitude = (ymin + ymax) / 2\n",
    "decimal_degrees = meters / (111319.9 * math.cos(latitude))\n",
    "\n",
    "# Negative buffer the data subset by 100 meters\n",
    "data_buffer = data_subset.buffer(decimal_degrees)\n",
    "\n",
    "# Convert polygon to polyline\n",
    "dataline = gpd.read_file(file)\n",
    "polyline = dataline.boundary\n",
    "\n",
    "# Subset the polyline GeoDataFrame to only include the first row\n",
    "polyline_subset = polyline.iloc[[i]]\n",
    "\n",
    "# Clip the polyline to the negative buffer of the data subset\n",
    "polyline_clip_breach = polyline_subset.intersection(data_buffer.geometry.iloc[0])\n",
    "\n",
    "# Remove the overlapping part between polyline_subset and polyline_clip_breach\n",
    "polyline_clip_held = polyline_subset.difference(polyline_clip_breach)\n",
    "\n",
    "# Create a 3x2 subplot figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Plot the polyline on the top subplot\n",
    "axs[0, 0].set_aspect(1)\n",
    "polyline_subset.plot(ax=axs[0, 0], color='black')\n",
    "axs[0, 0].set_title(\"Polyline - Current fire perimeter\")\n",
    "axs[0, 0].set_xlim([xmin, xmax])\n",
    "axs[0, 0].set_ylim([ymin, ymax])\n",
    "\n",
    "# Plot the polygon on the middle subplot\n",
    "axs[0, 1].set_aspect(1)\n",
    "data_subset.plot(ax=axs[0, 1], color='black')\n",
    "axs[0, 1].set_title(\"Polygon - Next day fire perimeter\")\n",
    "axs[0, 1].set_xlim([xmin, xmax])\n",
    "axs[0, 1].set_ylim([ymin, ymax])\n",
    "\n",
    "# Plot the clipped polyline on the bottom-left subplot\n",
    "axs[1, 0].set_aspect(1)\n",
    "polyline_clip_breach.plot(ax=axs[1, 0], color='red')\n",
    "axs[1, 0].set_title(\"Polyline - Breached\")\n",
    "axs[1, 0].set_xlim([xmin, xmax])\n",
    "axs[1, 0].set_ylim([ymin, ymax])\n",
    "\n",
    "# Plot the held polyline on the bottom-right subplot\n",
    "axs[1, 1].set_aspect(1)\n",
    "polyline_clip_held.plot(ax=axs[1, 1], color='green')\n",
    "axs[1, 1].set_title(\"Polyline - Held\")\n",
    "axs[1, 1].set_xlim([xmin, xmax])\n",
    "axs[1, 1].set_ylim([ymin, ymax])\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    \n",
    "#plt.savefig(\"C:\\\\PATH\\\\TO\\\\SAVE\\\\IMAGE\\\\fourpanel_breachheld.jpg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e22b0-892c-4076-b5c3-3dbdc752f3c5",
   "metadata": {},
   "source": [
    "# Identify breached points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04982d9a-c37c-4004-aa3c-049b0e24a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### CHANGE THIS ###\n",
    "PathToOutputShp = \"C:\\\\Users\\\\PATH\\\\TO\\\\OUTPUT\\\\SHP\\\\polyline_breach_PTS.shp\"\n",
    "\n",
    "# the approximate distance between each point (estimated based on decimal degree conversion)\n",
    "z=620\n",
    "\n",
    "# Convert MultiLineString to single LineString\n",
    "polyline_gdf = polyline_clip_breach.explode().reset_index(drop=True)\n",
    "\n",
    "# Create an empty list to store the points GeoDataFrames\n",
    "points_gdfs = []\n",
    "\n",
    "# Loop through the geometries in the GeoDataFrame\n",
    "for polyline_geometry in polyline_gdf.geometry:\n",
    "\n",
    "    # Create a LineString object from the polyline geometry\n",
    "    polyline_ls = LineString(polyline_geometry)\n",
    "\n",
    "    # Calculate the length of the line string in meters\n",
    "    line_length = polyline_ls.length\n",
    "    \n",
    "    # Trim short line segments blips\n",
    "    if line_length <= 0.01:\n",
    "        continue\n",
    "\n",
    "    # Calculate the bounds of the line string\n",
    "    x_coords = [p[0] for p in polyline_ls.coords]\n",
    "    y_coords = [p[1] for p in polyline_ls.coords]\n",
    "    xmin, ymin, xmax, ymax = min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "    \n",
    "    latitude = (ymin + ymax) / 2\n",
    "    meters = z\n",
    "    #decimal_degrees = -(meters / (111319.9 * math.cos(latitude)))\n",
    "    decimal_degrees = meters / (111319.9 * np.sqrt(1 - 0.00669438 * np.sin(latitude)**2))\n",
    "    # Calculate the number of points needed to space them every 60 meters\n",
    "    #num_points = int(line_length // 0.001) + 1\n",
    "\n",
    "    # Create a list of points spaced every 60 meters along the line string\n",
    "    #points = [polyline_ls.interpolate(i * 0.001) for i in range(num_points)]\n",
    "    \n",
    "    # Calculate the number of points needed to space them every 60 meters\n",
    "    num_points = int(line_length // decimal_degrees) + 1\n",
    "\n",
    "    # Create a list of points spaced every 60 meters along the line string\n",
    "    points = [polyline_ls.interpolate(i * decimal_degrees) for i in range(num_points)]\n",
    "\n",
    "    # Convert the points to a GeoDataFrame\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=[Point(p.x, p.y) for p in points], crs=polyline_gdf.crs)\n",
    "\n",
    "    # Append the points GeoDataFrame to the list\n",
    "    points_gdfs.append(points_gdf)\n",
    "\n",
    "# Concatenate all the points GeoDataFrames into a single GeoDataFrame\n",
    "points_gdf = pd.concat(points_gdfs)\n",
    "\n",
    "# Save the points as a shapefile\n",
    "points_gdf.to_file(PathToOutputShp)\n",
    "print(\"pts complete\")\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the points\n",
    "points_gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "\n",
    "# Optionally, plot the original line for context\n",
    "polyline_gdf.plot(ax=ax, linewidth=1, color='blue')\n",
    "\n",
    "# Set the title and axes labels\n",
    "ax.set_title('Points spaced along polyline')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcd3c0-ab89-4a5b-abae-10e1a4606903",
   "metadata": {},
   "source": [
    "# Identify held points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67bda1-e468-45a2-bc63-c59a599e41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "### CHANGE THIS ###\n",
    "PathToOutputShp = \"C:\\\\Users\\\\PATH\\\\TO\\\\OUTPUT\\\\SHP\\\\polyline_held_PTS.shp\"\n",
    "\n",
    "# the approximate distance between each point\n",
    "z=620\n",
    "\n",
    "# Convert MultiLineString to single LineString\n",
    "polyline_gdf = polyline_clip_held.explode().reset_index(drop=True)\n",
    "\n",
    "# Create an empty list to store the points GeoDataFrames\n",
    "points_gdfs = []\n",
    "\n",
    "# Loop through the geometries in the GeoDataFrame\n",
    "for polyline_geometry in polyline_gdf.geometry:\n",
    "\n",
    "    # Create a LineString object from the polyline geometry\n",
    "    polyline_ls = LineString(polyline_geometry)\n",
    "\n",
    "    # Calculate the length of the line string in meters\n",
    "    line_length = polyline_ls.length\n",
    "    \n",
    "    # Trim short line segments blips\n",
    "    if line_length <= 0.01:\n",
    "        continue\n",
    "\n",
    "    # Calculate the bounds of the line string\n",
    "    x_coords = [p[0] for p in polyline_ls.coords]\n",
    "    y_coords = [p[1] for p in polyline_ls.coords]\n",
    "    xmin, ymin, xmax, ymax = min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "    \n",
    "    latitude = (ymin + ymax) / 2\n",
    "    meters = z\n",
    "    #decimal_degrees = -(meters / (111319.9 * math.cos(latitude)))\n",
    "    decimal_degrees = meters / (111319.9 * np.sqrt(1 - 0.00669438 * np.sin(latitude)**2))\n",
    "    # Calculate the number of points needed to space them every 60 meters\n",
    "    #num_points = int(line_length // 0.001) + 1\n",
    "\n",
    "    # Create a list of points spaced every 60 meters along the line string\n",
    "    #points = [polyline_ls.interpolate(i * 0.001) for i in range(num_points)]\n",
    "    \n",
    "    # Calculate the number of points needed to space them every 60 meters\n",
    "    num_points = int(line_length // decimal_degrees) + 1\n",
    "\n",
    "    # Create a list of points spaced every 60 meters along the line string\n",
    "    points = [polyline_ls.interpolate(i * decimal_degrees) for i in range(num_points)]\n",
    "\n",
    "    # Convert the points to a GeoDataFrame\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=[Point(p.x, p.y) for p in points], crs=polyline_gdf.crs)\n",
    "\n",
    "    # Append the points GeoDataFrame to the list\n",
    "    points_gdfs.append(points_gdf)\n",
    "\n",
    "# Concatenate all the points GeoDataFrames into a single GeoDataFrame\n",
    "points_gdf = pd.concat(points_gdfs)\n",
    "\n",
    "# Save the points as a shapefile\n",
    "points_gdf.to_file(PathToOutputShp)\n",
    "print(\"pts complete\")\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the points\n",
    "points_gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "\n",
    "# Optionally, plot the original line for context\n",
    "polyline_gdf.plot(ax=ax, linewidth=1, color='blue')\n",
    "\n",
    "# Set the title and axes labels\n",
    "ax.set_title('Points spaced along polyline')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dbbe8-e97c-448a-9b27-9d57e13059b8",
   "metadata": {},
   "source": [
    "# Weather and topograph data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e81a9-8b4c-4166-8730-400b78954c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = \"C:\\\\PATH\\\\TO\\\\FIRESPREAD\\\\SHP\\\\FireSpreadPoly_4days_simple.shp\"\n",
    "\n",
    "datalength = len(gpd.read_file(file))\n",
    "print(int(datalength))\n",
    "\n",
    "i=0\n",
    "w=1\n",
    "\n",
    "df_list_breach = []\n",
    "df_list_held = []\n",
    "\n",
    "while w < int(datalength):\n",
    "    # Day current \n",
    "    # Day Next\n",
    "\n",
    "    # temp path for ee library, this was created above\n",
    "    PathToOutputShpBreach = \"C:\\\\Users\\\\PATH\\\\TO\\\\OUTPUT\\\\SHP\\\\polyline_breach_PTS.shp\"\n",
    "    PathToOutputShpHeld = \"C:\\\\Users\\\\PATH\\\\TO\\\\OUTPUT\\\\SHP\\\\polyline_held_PTS.shp\"\n",
    "\n",
    "    # buffer to remove edge effects \n",
    "    meters = 300\n",
    "\n",
    "    # the approximate distance (m) between each point along the perimeter\n",
    "    z=620\n",
    "\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    # identify breach and held locations from i to w\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    #########################\n",
    "    # Read in the shapefile #\n",
    "    data = gpd.read_file(file)\n",
    "    data = data.sort_values('GDB_TO_DAT')\n",
    "\n",
    "    # Subset the data GeoDataFrame to only include the first row\n",
    "    data_subset = data.iloc[[w]]\n",
    "\n",
    "    xmin, ymin, xmax, ymax = data.total_bounds\n",
    "    latitude = (ymin + ymax) / 2\n",
    "    decimal_degrees = meters / (111319.9 * math.cos(latitude))\n",
    "\n",
    "    # Negative buffer the data subset by 100 meters\n",
    "    data_buffer = data_subset.buffer(decimal_degrees)\n",
    "\n",
    "    # Convert polygon to polyline\n",
    "    dataline = gpd.read_file(file)\n",
    "    polyline = dataline.boundary\n",
    "\n",
    "    # Subset the polyline GeoDataFrame to only include the first row\n",
    "    polyline_subset = polyline.iloc[[i]]\n",
    "\n",
    "    # Clip the polyline to the negative buffer of the data subset\n",
    "    polyline_clip_breach = polyline_subset.intersection(data_buffer.geometry.iloc[0])\n",
    "\n",
    "    # Remove the overlapping part between polyline_subset and polyline_clip_breach\n",
    "    polyline_clip_held = polyline_subset.difference(polyline_clip_breach)\n",
    "\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    # polyline_breach_PTS processing\n",
    "\n",
    "    # Convert MultiLineString to single LineString\n",
    "    polyline_gdf = polyline_clip_breach.explode().reset_index(drop=True)\n",
    "\n",
    "    # Create an empty list to store the points GeoDataFrames\n",
    "    points_gdfs = []\n",
    "\n",
    "    # Loop through the geometries in the GeoDataFrame\n",
    "    for polyline_geometry in polyline_gdf.geometry:\n",
    "\n",
    "        # Create a LineString object from the polyline geometry\n",
    "        polyline_ls = LineString(polyline_geometry)\n",
    "\n",
    "        # Calculate the length of the line string in meters\n",
    "        line_length = polyline_ls.length\n",
    "\n",
    "        # Trim short line segments blips\n",
    "        if line_length <= 0.01:\n",
    "            continue\n",
    "\n",
    "        # Calculate the bounds of the line string\n",
    "        x_coords = [p[0] for p in polyline_ls.coords]\n",
    "        y_coords = [p[1] for p in polyline_ls.coords]\n",
    "        xmin, ymin, xmax, ymax = min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "\n",
    "        latitude = (ymin + ymax) / 2\n",
    "        meters = z\n",
    "        #decimal_degrees = -(meters / (111319.9 * math.cos(latitude)))\n",
    "        decimal_degrees = meters / (111319.9 * np.sqrt(1 - 0.00669438 * np.sin(latitude)**2))\n",
    "        # Calculate the number of points needed to space them every 60 meters\n",
    "        #num_points = int(line_length // 0.001) + 1\n",
    "\n",
    "        # Create a list of points spaced every 60 meters along the line string\n",
    "        #points = [polyline_ls.interpolate(i * 0.001) for i in range(num_points)]\n",
    "\n",
    "        # Calculate the number of points needed to space them every 60 meters\n",
    "        num_points = int(line_length // decimal_degrees) + 1\n",
    "\n",
    "        # Create a list of points spaced every 60 meters along the line string\n",
    "        points = [polyline_ls.interpolate(i * decimal_degrees) for i in range(num_points)]\n",
    "\n",
    "        # Convert the points to a GeoDataFrame\n",
    "        points_gdf = gpd.GeoDataFrame(geometry=[Point(p.x, p.y) for p in points], crs=polyline_gdf.crs)\n",
    "\n",
    "        # Append the points GeoDataFrame to the list\n",
    "        points_gdfs.append(points_gdf)\n",
    "\n",
    "    # Concatenate all the points GeoDataFrames into a single GeoDataFrame\n",
    "    points_gdf = pd.concat(points_gdfs)\n",
    "\n",
    "    # Save the points as a shapefile\n",
    "    points_gdf.to_file(PathToOutputShpBreach)\n",
    "    print(\"pts complete\")\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    #\n",
    "    ##\n",
    "    ###\n",
    "    ##\n",
    "    #\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    # polyline_held_PTS processing\n",
    "\n",
    "    # Convert MultiLineString to single LineString\n",
    "    polyline_gdf = polyline_clip_held.explode().reset_index(drop=True)\n",
    "\n",
    "    # Create an empty list to store the points GeoDataFrames\n",
    "    points_gdfs = []\n",
    "\n",
    "    # Loop through the geometries in the GeoDataFrame\n",
    "    for polyline_geometry in polyline_gdf.geometry:\n",
    "\n",
    "        # Create a LineString object from the polyline geometry\n",
    "        polyline_ls = LineString(polyline_geometry)\n",
    "\n",
    "        # Calculate the length of the line string in meters\n",
    "        line_length = polyline_ls.length\n",
    "\n",
    "        # Trim short line segments blips\n",
    "        if line_length <= 0.01:\n",
    "            continue\n",
    "\n",
    "        # Calculate the bounds of the line string\n",
    "        x_coords = [p[0] for p in polyline_ls.coords]\n",
    "        y_coords = [p[1] for p in polyline_ls.coords]\n",
    "        xmin, ymin, xmax, ymax = min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "\n",
    "        latitude = (ymin + ymax) / 2\n",
    "        meters = z\n",
    "        #decimal_degrees = -(meters / (111319.9 * math.cos(latitude)))\n",
    "        decimal_degrees = meters / (111319.9 * np.sqrt(1 - 0.00669438 * np.sin(latitude)**2))\n",
    "        # Calculate the number of points needed to space them every 60 meters\n",
    "        #num_points = int(line_length // 0.001) + 1\n",
    "\n",
    "        # Create a list of points spaced every 60 meters along the line string\n",
    "        #points = [polyline_ls.interpolate(i * 0.001) for i in range(num_points)]\n",
    "\n",
    "        # Calculate the number of points needed to space them every 60 meters\n",
    "        num_points = int(line_length // decimal_degrees) + 1\n",
    "\n",
    "        # Create a list of points spaced every 60 meters along the line string\n",
    "        points = [polyline_ls.interpolate(i * decimal_degrees) for i in range(num_points)]\n",
    "\n",
    "        # Convert the points to a GeoDataFrame\n",
    "        points_gdf = gpd.GeoDataFrame(geometry=[Point(p.x, p.y) for p in points], crs=polyline_gdf.crs)\n",
    "\n",
    "        # Append the points GeoDataFrame to the list\n",
    "        points_gdfs.append(points_gdf)\n",
    "\n",
    "    # Concatenate all the points GeoDataFrames into a single GeoDataFrame\n",
    "    points_gdf = pd.concat(points_gdfs)\n",
    "\n",
    "    # Save the points as a shapefile\n",
    "    points_gdf.to_file(PathToOutputShpHeld)\n",
    "    print(\"pts complete\")\n",
    "    \n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    ee_fc_breach = geemap.shp_to_ee(PathToOutputShpBreach)\n",
    "    ee_fc_held = geemap.shp_to_ee(PathToOutputShpHeld)\n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    # terrain breach\n",
    "    # Load the 3DEP dataset  and others and filter by the extent of the points\n",
    "    dem = ee.Image(\"USGS/3DEP/10m\").clip(ee_fc_breach.geometry().bounds())\n",
    "    topo_diversity = ee.Image(\"CSP/ERGo/1_0/US/topoDiversity\").clip(ee_fc_breach.geometry().bounds())\n",
    "    physiography = ee.Image(\"CSP/ERGo/1_0/US/physiography\").clip(ee_fc_breach.geometry().bounds())\n",
    "    physio_diversity = ee.Image(\"CSP/ERGo/1_0/US/physioDiversity\").clip(ee_fc_breach.geometry().bounds())\n",
    "    mtpi = ee.Image(\"CSP/ERGo/1_0/US/mTPI\").clip(ee_fc_breach.geometry().bounds())\n",
    "    lithology = ee.Image(\"CSP/ERGo/1_0/US/lithology\").clip(ee_fc_breach.geometry().bounds())\n",
    "    landforms = ee.Image(\"CSP/ERGo/1_0/US/landforms\").clip(ee_fc_breach.geometry().bounds())\n",
    "    chili = ee.Image(\"CSP/ERGo/1_0/US/CHILI\").clip(ee_fc_breach.geometry().bounds())\n",
    "\n",
    "    # Extract data for each image at appropriate scale\n",
    "    terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc_breach, scale=10)\n",
    "\n",
    "    topo_diversity_fc = topo_diversity.sampleRegions(collection=ee_fc_breach, scale=90)\n",
    "    physiography_fc = physiography.sampleRegions(collection=ee_fc_breach, scale=90)\n",
    "    physio_diversity_fc = physio_diversity.sampleRegions(collection=ee_fc_breach, scale=270)\n",
    "    mtpi_fc = mtpi.sampleRegions(collection=ee_fc_breach, scale=270)\n",
    "    lithology_fc = lithology.sampleRegions(collection=ee_fc_breach, scale=90)\n",
    "    landforms_fc = landforms.sampleRegions(collection=ee_fc_breach, scale=10)\n",
    "    chili_fc = chili.sampleRegions(collection=ee_fc_breach, scale=10)\n",
    "\n",
    "    # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "    terrain_df = geemap.ee_to_df(terrain_fc)\n",
    "\n",
    "    topo_diversity_df = geemap.ee_to_df(topo_diversity_fc)\n",
    "    physiography_df = geemap.ee_to_df(physiography_fc)\n",
    "    physio_diversity_df = geemap.ee_to_df(physio_diversity_fc)\n",
    "    mtpi_df = geemap.ee_to_df(mtpi_fc)\n",
    "    lithology_df = geemap.ee_to_df(lithology_fc)\n",
    "    landforms_df = geemap.ee_to_df(landforms_fc)\n",
    "    chili_df = geemap.ee_to_df(chili_fc)\n",
    "\n",
    "    # Rename columns to appropriate names\n",
    "    topo_diversity_df = topo_diversity_df.rename(columns={'constant': 'topo_diversity'})\n",
    "    physiography_df = physiography_df.rename(columns={'constant': 'physiography'})\n",
    "    physio_diversity_df = physio_diversity_df.rename(columns={'b1': 'physio_diversity'})\n",
    "    mtpi_df = mtpi_df.rename(columns={'elevation': 'mtpi'})\n",
    "    lithology_df = lithology_df.rename(columns={'b1': 'lithology'})\n",
    "    landforms_df = landforms_df.rename(columns={'constant': 'landforms'})\n",
    "    chili_df = chili_df.rename(columns={'constant': 'chili'})\n",
    "    \n",
    "    ##########################\n",
    "    ##### Breach Weather #####\n",
    "    ##########################\n",
    "\n",
    "    start_date_str = str(data.GDB_TO_DAT[i])\n",
    "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "\n",
    "    era5 = ee.ImageCollection(\"ECMWF/ERA5/HOURLY\").select(['u_component_of_wind_10m', 'v_component_of_wind_10m', \n",
    "                                                      'dewpoint_temperature_2m', 'temperature_2m', 'skin_temperature',\n",
    "                                                      'soil_temperature_level_1', 'volumetric_soil_water_layer_1',\n",
    "                                                      'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3',\n",
    "                                                      'volumetric_soil_water_layer_4', 'forecast_albedo',\n",
    "                                                      'surface_latent_heat_flux', 'surface_net_solar_radiation',\n",
    "                                                      'surface_net_thermal_radiation',\n",
    "                                                      'potential_evaporation',\n",
    "                                                      'total_precipitation', 'leaf_area_index_high_vegetation',\n",
    "                                                      'leaf_area_index_low_vegetation']).filterDate(start_date, end_date)\n",
    "\n",
    "    \n",
    "    era5mean_fc = era5.mean().reduceRegions(collection=ee_fc_breach, reducer=ee.Reducer.mean(), scale=11132)\n",
    "    era5mean_df = geemap.ee_to_df(era5mean_fc)\n",
    "\n",
    "    print(era5mean_df)\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    ### END Breach Weather ###\n",
    "    ##########################\n",
    "\n",
    "    df = pd.concat([terrain_df, topo_diversity_df.iloc[:, 1], physiography_df.iloc[:, 1], physio_diversity_df.iloc[:, 1], \n",
    "                mtpi_df.iloc[:, 1], lithology_df.iloc[:, 1], landforms_df.iloc[:, 1], chili_df.iloc[:, 1], era5mean_df], axis=1)\n",
    "    df_breach = df.dropna()\n",
    "    df_breach[\"date\"] = data.GDB_TO_DAT[i]\n",
    "    df_breach[\"numberofpoints\"] = len(df_breach)\n",
    "\n",
    "    print(\"df_breach length is: \", len(df_breach))\n",
    "    #df_breach\n",
    "    \n",
    "    #############################################################################################################################\n",
    "    #############################################################################################################################\n",
    "    # terrain held\n",
    "    # Load the 3DEP dataset and filter by the extent of the points\n",
    "    dem = ee.Image(\"USGS/3DEP/10m\").clip(ee_fc_held.geometry().bounds())\n",
    "    topo_diversity = ee.Image(\"CSP/ERGo/1_0/US/topoDiversity\").clip(ee_fc_held.geometry().bounds())\n",
    "    physiography = ee.Image(\"CSP/ERGo/1_0/US/physiography\").clip(ee_fc_held.geometry().bounds())\n",
    "    physio_diversity = ee.Image(\"CSP/ERGo/1_0/US/physioDiversity\").clip(ee_fc_held.geometry().bounds())\n",
    "    mtpi = ee.Image(\"CSP/ERGo/1_0/US/mTPI\").clip(ee_fc_held.geometry().bounds())\n",
    "    lithology = ee.Image(\"CSP/ERGo/1_0/US/lithology\").clip(ee_fc_held.geometry().bounds())\n",
    "    landforms = ee.Image(\"CSP/ERGo/1_0/US/landforms\").clip(ee_fc_held.geometry().bounds())\n",
    "    chili = ee.Image(\"CSP/ERGo/1_0/US/CHILI\").clip(ee_fc_held.geometry().bounds())\n",
    "\n",
    "    # Extract data for each image at appropriate scale\n",
    "    terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc_held, scale=10)\n",
    "\n",
    "    topo_diversity_fc = topo_diversity.sampleRegions(collection=ee_fc_held, scale=90)\n",
    "    physiography_fc = physiography.sampleRegions(collection=ee_fc_held, scale=90)\n",
    "    physio_diversity_fc = physio_diversity.sampleRegions(collection=ee_fc_held, scale=270)\n",
    "    mtpi_fc = mtpi.sampleRegions(collection=ee_fc_held, scale=270)\n",
    "    lithology_fc = lithology.sampleRegions(collection=ee_fc_held, scale=90)\n",
    "    landforms_fc = landforms.sampleRegions(collection=ee_fc_held, scale=10)\n",
    "    chili_fc = chili.sampleRegions(collection=ee_fc_held, scale=10)\n",
    "\n",
    "    # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "    terrain_df = geemap.ee_to_df(terrain_fc)\n",
    "\n",
    "    topo_diversity_df = geemap.ee_to_df(topo_diversity_fc)\n",
    "    physiography_df = geemap.ee_to_df(physiography_fc)\n",
    "    physio_diversity_df = geemap.ee_to_df(physio_diversity_fc)\n",
    "    mtpi_df = geemap.ee_to_df(mtpi_fc)\n",
    "    lithology_df = geemap.ee_to_df(lithology_fc)\n",
    "    landforms_df = geemap.ee_to_df(landforms_fc)\n",
    "    chili_df = geemap.ee_to_df(chili_fc)\n",
    "\n",
    "    # Rename columns to appropriate names\n",
    "    topo_diversity_df = topo_diversity_df.rename(columns={'constant': 'topo_diversity'})\n",
    "    physiography_df = physiography_df.rename(columns={'constant': 'physiography'})\n",
    "    physio_diversity_df = physio_diversity_df.rename(columns={'b1': 'physio_diversity'})\n",
    "    mtpi_df = mtpi_df.rename(columns={'elevation': 'mtpi'})\n",
    "    lithology_df = lithology_df.rename(columns={'b1': 'lithology'})\n",
    "    landforms_df = landforms_df.rename(columns={'constant': 'landforms'})\n",
    "    chili_df = chili_df.rename(columns={'constant': 'chili'})\n",
    "\n",
    "    ##########################\n",
    "    ##### Held Weather #####\n",
    "    ##########################\n",
    "\n",
    "    start_date_str = str(data.GDB_TO_DAT[i])\n",
    "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "\n",
    "    era5 = ee.ImageCollection(\"ECMWF/ERA5/HOURLY\").select(['u_component_of_wind_10m', 'v_component_of_wind_10m', \n",
    "                                                      'dewpoint_temperature_2m', 'temperature_2m', 'skin_temperature',\n",
    "                                                      'soil_temperature_level_1', 'volumetric_soil_water_layer_1',\n",
    "                                                      'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3',\n",
    "                                                      'volumetric_soil_water_layer_4', 'forecast_albedo',\n",
    "                                                      'surface_latent_heat_flux', 'surface_net_solar_radiation',\n",
    "                                                      'surface_net_thermal_radiation',\n",
    "                                                      'potential_evaporation',\n",
    "                                                      'total_precipitation', 'leaf_area_index_high_vegetation',\n",
    "                                                      'leaf_area_index_low_vegetation']).filterDate(start_date, end_date)\n",
    "\n",
    "    \n",
    "    era5mean_fc = era5.mean().reduceRegions(collection=ee_fc_held, reducer=ee.Reducer.mean(), scale=11132)\n",
    "    era5mean_df = geemap.ee_to_df(era5mean_fc)\n",
    "\n",
    "    ##########################\n",
    "    ### END Held Weather ###\n",
    "    ##########################\n",
    "    \n",
    "    #dfs = [terrain_df, topo_diversity_df, physiography_df, physio_diversity_df, mtpi_df, lithology_df, landforms_df, chili_df]\n",
    "    df = pd.concat([terrain_df, topo_diversity_df.iloc[:, 1], physiography_df.iloc[:, 1], physio_diversity_df.iloc[:, 1], \n",
    "                    mtpi_df.iloc[:, 1], lithology_df.iloc[:, 1], landforms_df.iloc[:, 1], chili_df.iloc[:, 1], era5mean_df], axis=1)\n",
    "\n",
    "    df_held = df.dropna()\n",
    "    df_held[\"date\"] = data.GDB_TO_DAT[i]\n",
    "    df_held[\"numberofpoints\"] = len(df_held)\n",
    "\n",
    "\n",
    "    print(\"df_held length is: \",len(df_held))\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(\"Total time:\", total_time, \" seconds\")\n",
    "    \n",
    "    i=i+1\n",
    "    w=w+1\n",
    "    \n",
    "    df_list_held.append(df_held)\n",
    "    df_list_breach.append(df_breach)\n",
    "\n",
    "\n",
    "held_df = pd.concat(df_list_held, ignore_index=True)\n",
    "breach_df = pd.concat(df_list_breach, ignore_index=True)\n",
    "\n",
    "held_df['binary'] = 0\n",
    "breach_df['binary'] = 1\n",
    "df = pd.concat([held_df, breach_df], axis=0)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe2716-1c09-46f2-b188-663f504a336d",
   "metadata": {},
   "source": [
    "# Survival Analysis Using Kaplan-Meier Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd3496-9685-40c0-8674-40cec22b782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "import pandas as pd\n",
    "\n",
    "reference_date = pd.to_datetime(df['date']).min()  # This sets the earliest date in your data as a reference point.\n",
    "print(reference_date)\n",
    "df['duration'] = (pd.to_datetime(df['date']) - reference_date).dt.days  # This calculates the duration in days from the reference date.\n",
    "\n",
    "\n",
    "# Create a Kaplan-Meier estimator\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Fit the Kaplan-Meier estimator using the 'binary' column as the event indicator\n",
    "kmf.fit(df['duration'], event_observed=df['binary'], label='Kaplan-Meier Estimate')\n",
    "\n",
    "# Plot the Kaplan-Meier survival curve\n",
    "kmf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c7acb-01c5-40f2-9910-c3f08cda3335",
   "metadata": {},
   "source": [
    "# Cox Proportional Hazards Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25a8a0-f979-4879-8e0a-df5fed9a3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# Assuming 'df' is already defined and contains your data\n",
    "# Convert 'date' from a datetime to a numeric type representing the duration\n",
    "reference_date = pd.to_datetime(df['date']).min()\n",
    "df['duration'] = (pd.to_datetime(df['date']) - reference_date).dt.days\n",
    "\n",
    "# 'binary' column is already set up correctly as the event indicator\n",
    "\n",
    "# Select the features to include in the Cox model\n",
    "# Add or remove columns as necessary for your analysis\n",
    "columns = [\n",
    "    'aspect', 'elevation', 'u_component_of_wind_10m','surface_latent_heat_flux',\n",
    "    'v_component_of_wind_10m', 'duration', 'chili', 'dewpoint_temperature_2m',\n",
    "     'leaf_area_index_high_vegetation', 'leaf_area_index_low_vegetation','binary'\n",
    "]\n",
    "\n",
    "# Ensure all columns are included in the DataFrame before fitting the model\n",
    "X = df[columns]\n",
    "\n",
    "# Initialize and fit the Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(X, duration_col='duration', event_col='binary', show_progress=True)\n",
    "\n",
    "# Print the summary of the Cox model\n",
    "cph.summary\n",
    "\n",
    "# Plot the survival curves for the Cox model\n",
    "cph.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
